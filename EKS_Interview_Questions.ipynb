{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab6d7745",
   "metadata": {},
   "source": [
    "# Production EKS Deployment - Interview Questions & Answers\n",
    "\n",
    "## Overview\n",
    "This notebook covers common interview questions and detailed answers for a production-grade ML inference service deployed on AWS EKS with Terraform, Kubernetes, and FastAPI.\n",
    "\n",
    "**Project Context:**\n",
    "- ML Model: Random Forest Classifier (scikit-learn)\n",
    "- API Framework: FastAPI with Uvicorn\n",
    "- Container Registry: AWS ECR\n",
    "- Orchestration: AWS EKS (Elastic Kubernetes Service)\n",
    "- Infrastructure: Terraform\n",
    "- Region: ap-southeast-2 (Sydney)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8671d837",
   "metadata": {},
   "source": [
    "# 1. KUBERNETES FUNDAMENTALS\n",
    "\n",
    "## Q1.1: Explain the difference between Deployment, StatefulSet, and DaemonSet\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**Deployment:**\n",
    "- For **stateless applications** (like our ML API)\n",
    "- Pods are interchangeable and can be scaled up/down\n",
    "- Manages ReplicaSets for rolling updates\n",
    "- Our use case: ML inference service with 2-10 replicas\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: ml-inference-api\n",
    "spec:\n",
    "  replicas: 2  # Managed by HPA (2-10)\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: ml-inference\n",
    "```\n",
    "\n",
    "**StatefulSet:**\n",
    "- For **stateful applications** (databases, caches)\n",
    "- Pods have stable, unique identities (pod-0, pod-1, pod-2)\n",
    "- Maintains persistent storage\n",
    "- Example: PostgreSQL database with persistent volumes\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: StatefulSet\n",
    "metadata:\n",
    "  name: postgres\n",
    "spec:\n",
    "  serviceName: \"postgres\"\n",
    "  replicas: 3\n",
    "  volumeClaimTemplates:\n",
    "    - metadata:\n",
    "        name: data\n",
    "      spec:\n",
    "        accessModes: [ \"ReadWriteOnce\" ]\n",
    "        resources:\n",
    "          requests:\n",
    "            storage: 10Gi\n",
    "```\n",
    "\n",
    "**DaemonSet:**\n",
    "- Ensures **one pod per node** (or per selected nodes)\n",
    "- Used for cluster monitoring, logging, networking\n",
    "- Examples: Prometheus node exporter, Fluentd, Calico\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: DaemonSet\n",
    "metadata:\n",
    "  name: prometheus-node-exporter\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: prometheus-node-exporter\n",
    "  template:\n",
    "    # Pod template automatically deployed to all nodes\n",
    "```\n",
    "\n",
    "### Key Differences Table:\n",
    "\n",
    "| Aspect | Deployment | StatefulSet | DaemonSet |\n",
    "|--------|-----------|------------|----------|\n",
    "| Use Case | Stateless apps | Stateful apps | Node-level services |\n",
    "| Pod Identity | Interchangeable | Stable & unique | One per node |\n",
    "| Scaling | Any number | Ordered, unique | Fixed per topology |\n",
    "| Storage | No persistent | Persistent volumes | Host storage |\n",
    "| Examples | APIs, web servers | Databases, Kafka | Monitoring, logging |\n",
    "| Our Usage | ✓ ML API | ✗ | ✗ |\n",
    "\n",
    "### Follow-up Questions:\n",
    "- Why can't you scale a StatefulSet as easily as a Deployment?\n",
    "- What happens if you delete a pod in a StatefulSet?\n",
    "- Why would you use a DaemonSet instead of a Deployment with node affinity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b944437b",
   "metadata": {},
   "source": [
    "## Q1.2: Explain Kubernetes Services and the difference between ClusterIP, NodePort, LoadBalancer, and ExternalName\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**Kubernetes Service** = Network abstraction that provides stable endpoint for pods\n",
    "\n",
    "**ClusterIP (Default):**\n",
    "- Only accessible **within the cluster**\n",
    "- Internal communication between pods\n",
    "- Our use case: `ml-inference-service` for internal pod-to-pod communication\n",
    "- IP is stable but only routable within the cluster VPC\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: ml-inference-service\n",
    "spec:\n",
    "  type: ClusterIP\n",
    "  ports:\n",
    "    - port: 5000\n",
    "      targetPort: 5000\n",
    "  selector:\n",
    "    app: ml-inference\n",
    "```\n",
    "\n",
    "**NodePort:**\n",
    "- Opens port on **every node** (30000-32767)\n",
    "- Can access from outside: `<node-ip>:<node-port>`\n",
    "- Problems: Requires managing many node IPs, port conflicts\n",
    "- Use case: Development, testing, legacy systems\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: ml-inference-nodeport\n",
    "spec:\n",
    "  type: NodePort\n",
    "  ports:\n",
    "    - port: 5000\n",
    "      targetPort: 5000\n",
    "      nodePort: 30500  # Fixed port on all nodes\n",
    "```\n",
    "\n",
    "**LoadBalancer:**\n",
    "- Provisions **cloud provider load balancer** (AWS NLB/ALB in our case)\n",
    "- Single public IP/hostname for external access\n",
    "- Our use case: `ml-inference-lb` service\n",
    "- Best for: Production APIs needing external access\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: ml-inference-lb\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  ports:\n",
    "    - port: 80\n",
    "      targetPort: 5000\n",
    "  selector:\n",
    "    app: ml-inference\n",
    "```\n",
    "\n",
    "**ExternalName:**\n",
    "- Creates DNS CNAME record pointing to external service\n",
    "- Use case: Access external database, legacy systems\n",
    "- No load balancing, just DNS redirection\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: external-database\n",
    "spec:\n",
    "  type: ExternalName\n",
    "  externalName: database.example.com\n",
    "  ports:\n",
    "    - port: 5432\n",
    "```\n",
    "\n",
    "### Architecture in Our Setup:\n",
    "\n",
    "```\n",
    "Internet\n",
    "  ↓\n",
    "AWS NLB (LoadBalancer service, port 80)\n",
    "  ↓\n",
    "Kubernetes Node 1           Kubernetes Node 2\n",
    "  ├─ Pod ml-inference-api   ├─ Pod ml-inference-api\n",
    "  ├─ Pod ml-inference-api   └─ Pod ml-inference-api\n",
    "  └─ Pod ml-inference-api\n",
    "  \n",
    "All pods accessible via:\n",
    "- ClusterIP: ml-inference-service:5000 (internal)\n",
    "- LoadBalancer: <NLB-IP>:80 (external)\n",
    "```\n",
    "\n",
    "### Follow-up:\n",
    "- What happens to existing connections when you scale down pods?\n",
    "- How does service discovery work in Kubernetes?\n",
    "- Why use both ClusterIP and LoadBalancer services?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba44307",
   "metadata": {},
   "source": [
    "## Q1.3: Explain Kubernetes Ingress and when to use it vs LoadBalancer Service\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**Ingress:**\n",
    "- **Layer 7 (Application layer)** routing\n",
    "- Host-based and path-based routing\n",
    "- Multiple services behind single IP\n",
    "- Requires Ingress Controller (AWS ALB Controller in our case)\n",
    "- Cost-effective for multiple services\n",
    "\n",
    "```yaml\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: ml-inference-ingress\n",
    "  annotations:\n",
    "    kubernetes.io/ingress.class: alb\n",
    "    alb.ingress.kubernetes.io/scheme: internet-facing\n",
    "spec:\n",
    "  rules:\n",
    "  - host: api.ml-inference.example.com\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: ml-inference-service\n",
    "            port:\n",
    "              number: 5000\n",
    "```\n",
    "\n",
    "**LoadBalancer Service:**\n",
    "- **Layer 4 (Transport layer)** routing\n",
    "- Direct IP routing (TCP/UDP)\n",
    "- One load balancer per service\n",
    "- No routing rules (just port mapping)\n",
    "- Simpler but more expensive\n",
    "\n",
    "### Comparison:\n",
    "\n",
    "| Aspect | Ingress | LoadBalancer Service |\n",
    "|--------|---------|---------------------|\n",
    "| Layer | L7 (HTTP/HTTPS) | L4 (TCP/UDP) |\n",
    "| Load Balancer | Shared (ALB) | Dedicated per service (NLB) |\n",
    "| Host-based routing | ✓ Yes | ✗ No |\n",
    "| Path-based routing | ✓ Yes | ✗ No |\n",
    "| Cost | ✓ Lower (1 ALB for multiple services) | ✗ Higher (1 NLB per service) |\n",
    "| SSL/TLS | ✓ Easy (via annotations) | ✓ Easy (via service) |\n",
    "| Latency | Slightly higher (extra hop) | Lower (direct) |\n",
    "| Use Case | REST APIs, web apps | Databases, gaming, low latency |\n",
    "| Our Usage | ✓ Optional (for domain) | ✓ Primary method |\n",
    "\n",
    "### Our Architecture Decision:\n",
    "\n",
    "We use **LoadBalancer service** because:\n",
    "1. Simple single API service (not multiple)\n",
    "2. Direct L4 routing is sufficient\n",
    "3. Lower latency for inference requests\n",
    "4. Cost difference minimal for single service\n",
    "\n",
    "If we had multiple services (API, admin, webhook), we'd use **Ingress** instead:\n",
    "```\n",
    "api.ml-inference.com → ALB → api service\n",
    "admin.ml-inference.com → ALB → admin service\n",
    "webhook.ml-inference.com → ALB → webhook service\n",
    "```\n",
    "\n",
    "### Follow-up:\n",
    "- How would you implement path-based routing (e.g., /api vs /health)?\n",
    "- Can you use both Ingress and LoadBalancer together?\n",
    "- How do you handle SSL/TLS with Ingress?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fc8ef2",
   "metadata": {},
   "source": [
    "# 2. KUBERNETES ADVANCED TOPICS\n",
    "\n",
    "## Q2.1: Explain health checks in Kubernetes: Liveness, Readiness, and Startup Probes\n",
    "\n",
    "### Answer:\n",
    "\n",
    "These three probe types help Kubernetes manage pod lifecycle:\n",
    "\n",
    "**Startup Probe:**\n",
    "- Checks if container has started successfully\n",
    "- Fails if startup takes too long (deadlock detection)\n",
    "- Once successful, switches to liveness/readiness checks\n",
    "- Our config: 30 checks × 10 seconds = 300 second max startup\n",
    "\n",
    "```yaml\n",
    "startupProbe:\n",
    "  httpGet:\n",
    "    path: /health\n",
    "    port: 5000\n",
    "  initialDelaySeconds: 0\n",
    "  periodSeconds: 10\n",
    "  failureThreshold: 30  # 30 × 10 = 5 min max startup\n",
    "```\n",
    "\n",
    "**Liveness Probe:**\n",
    "- Checks if container is **still alive**\n",
    "- If fails → Kubernetes restarts the pod\n",
    "- Detects deadlocks, infinite loops, memory leaks\n",
    "- Our config: Check every 10 seconds, restart after 3 failures\n",
    "\n",
    "```yaml\n",
    "livenessProbe:\n",
    "  httpGet:\n",
    "    path: /health\n",
    "    port: 5000\n",
    "  initialDelaySeconds: 30  # Wait 30 sec before first check\n",
    "  periodSeconds: 10        # Check every 10 sec\n",
    "  failureThreshold: 3      # Restart after 3 failures\n",
    "```\n",
    "\n",
    "**Readiness Probe:**\n",
    "- Checks if container is **ready to accept traffic**\n",
    "- If fails → Remove from load balancer (but don't restart)\n",
    "- Allows graceful degradation (e.g., during database migration)\n",
    "- Our config: Check every 5 seconds, remove from LB after 2 failures\n",
    "\n",
    "```yaml\n",
    "readinessProbe:\n",
    "  httpGet:\n",
    "    path: /health\n",
    "    port: 5000\n",
    "  initialDelaySeconds: 10  # Wait 10 sec before first check\n",
    "  periodSeconds: 5         # Check every 5 sec (more frequent)\n",
    "  failureThreshold: 2      # Remove from LB after 2 failures\n",
    "```\n",
    "\n",
    "### Timeline Example:\n",
    "\n",
    "```\n",
    "t=0s   Pod created\n",
    "t=0-300s   Startup probe running (allows 300s to initialize)\n",
    "t=30s  Startup successful → Liveness/Readiness probes begin\n",
    "t=30-50s  Readiness probe: 4 successful checks\n",
    "t=50s  Pod becomes READY, added to load balancer\n",
    "t=50s+ Continuous monitoring:\n",
    "        - Readiness: every 5 sec (fast detection of service degradation)\n",
    "        - Liveness: every 10 sec (detect crashes)\n",
    "\n",
    "If readiness fails:\n",
    "  → Pod removed from LB (no traffic sent)\n",
    "  → But pod stays running (app can recover)\n",
    "\n",
    "If liveness fails:\n",
    "  → Pod is restarted (killed and recreated)\n",
    "```\n",
    "\n",
    "### Probe Types in Our Implementation:\n",
    "\n",
    "```yaml\n",
    "containers:\n",
    "- name: ml-api\n",
    "  image: ml-inference-service:latest\n",
    "  ports:\n",
    "  - containerPort: 5000\n",
    "  \n",
    "  # Startup: Give app 5 min to initialize\n",
    "  startupProbe:\n",
    "    httpGet:\n",
    "      path: /health\n",
    "      port: 5000\n",
    "    failureThreshold: 30\n",
    "    periodSeconds: 10\n",
    "  \n",
    "  # Readiness: Quick detection of issues (every 5s)\n",
    "  readinessProbe:\n",
    "    httpGet:\n",
    "      path: /health\n",
    "      port: 5000\n",
    "    initialDelaySeconds: 10\n",
    "    periodSeconds: 5\n",
    "    failureThreshold: 2\n",
    "  \n",
    "  # Liveness: Restart if really dead (every 10s)\n",
    "  livenessProbe:\n",
    "    httpGet:\n",
    "      path: /health\n",
    "      port: 5000\n",
    "    initialDelaySeconds: 30\n",
    "    periodSeconds: 10\n",
    "    failureThreshold: 3\n",
    "```\n",
    "\n",
    "### Follow-up:\n",
    "- What happens if a pod fails readiness but passes liveness?\n",
    "- Why different `periodSeconds` for readiness (5) vs liveness (10)?\n",
    "- How would you implement a custom readiness check?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7f4d11",
   "metadata": {},
   "source": [
    "## Q2.2: Explain Horizontal Pod Autoscaler (HPA) and how it works\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**HPA** automatically scales pods based on metrics:\n",
    "\n",
    "### How HPA Works (Controller Loop):\n",
    "\n",
    "```\n",
    "Every 30 seconds (by default):\n",
    "\n",
    "1. Metrics Server collects pod metrics\n",
    "   ├─ CPU usage (millicores)\n",
    "   └─ Memory usage (bytes)\n",
    "\n",
    "2. HPA Controller reads metrics\n",
    "   └─ Calculates desired replicas based on policy\n",
    "\n",
    "3. Compare current vs desired\n",
    "   ├─ If CPU > threshold → Scale UP\n",
    "   ├─ If CPU < threshold → Scale DOWN (with cooldown)\n",
    "   └─ Otherwise → No change\n",
    "\n",
    "4. Update Deployment replicas\n",
    "   └─ Kubernetes creates/destroys pods\n",
    "```\n",
    "\n",
    "### Our HPA Configuration:\n",
    "\n",
    "```yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: ml-inference-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: ml-inference-api\n",
    "  \n",
    "  minReplicas: 2      # Always at least 2 for HA\n",
    "  maxReplicas: 10     # Max 10 to control costs\n",
    "  \n",
    "  # Metrics to monitor\n",
    "  metrics:\n",
    "  # Metric 1: CPU utilization\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70  # Scale up if avg CPU > 70%\n",
    "  \n",
    "  # Metric 2: Memory utilization\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80  # Scale up if avg memory > 80%\n",
    "  \n",
    "  # Scaling behavior\n",
    "  behavior:\n",
    "    scaleUp:\n",
    "      stabilizationWindowSeconds: 0  # Scale up immediately\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 100  # Double the pods (100% increase)\n",
    "        periodSeconds: 30\n",
    "      - type: Pods\n",
    "        value: 2    # Add 2 pods\n",
    "        periodSeconds: 30\n",
    "    \n",
    "    scaleDown:\n",
    "      stabilizationWindowSeconds: 300  # Wait 5 min before scaling down\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 50   # Remove 50% of pods\n",
    "        periodSeconds: 60\n",
    "```\n",
    "\n",
    "### Scaling Algorithm:\n",
    "\n",
    "```\n",
    "desiredReplicas = ceil(\n",
    "  currentReplicas × \n",
    "  (current_metric / target_metric)\n",
    ")\n",
    "\n",
    "Example:\n",
    "- Current: 2 replicas, each using 80% CPU\n",
    "- Average CPU: 80%\n",
    "- Target CPU: 70%\n",
    "- Desired: 2 × (80/70) = 2.28 → 3 replicas\n",
    "```\n",
    "\n",
    "### Timeline Example:\n",
    "\n",
    "```\n",
    "t=0min    API deployed with 2 replicas (min)\n",
    "          CPU per pod: 30%\n",
    "          All pods serving traffic\n",
    "\n",
    "t=5min    Traffic spike! 100 requests/sec\n",
    "          CPU per pod: 75% (above 70% threshold)\n",
    "          HPA detects: scale needed\n",
    "\n",
    "t=5:30min HPA creates 2 new pods (100% increase, max 2 pods)\n",
    "          Now 4 replicas total\n",
    "          CPU per pod: 40% (load distributed)\n",
    "\n",
    "t=6min    Traffic still high\n",
    "          CPU: 72% → Scale to 5 pods\n",
    "\n",
    "t=7min    Traffic peaks\n",
    "          CPU: 82% → Scale to 8 pods\n",
    "\n",
    "t=8min    Traffic reduces\n",
    "          CPU: 42% (below 70%)\n",
    "          HPA waits 5 min (stabilization window)\n",
    "\n",
    "t=13min   Traffic still low (CPU 40%)\n",
    "          Scale down: 8 × 0.5 = 4 pods\n",
    "\n",
    "t=14min   Scale down again: 4 × 0.5 = 2 pods (min)\n",
    "          Back to baseline\n",
    "```\n",
    "\n",
    "### Resource Requests/Limits (Critical for HPA):\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  requests:      # What pod needs to run\n",
    "    cpu: 250m    # Kubernetes reserves 250 millicores\n",
    "    memory: 512Mi # Kubernetes reserves 512 MB\n",
    "  \n",
    "  limits:        # Max allowed\n",
    "    cpu: 1000m   # Can burst up to 1 CPU\n",
    "    memory: 1Gi  # Can use up to 1 GB\n",
    "\n",
    "# HPA calculates: (actual usage / requests) × 100 = utilization %\n",
    "# Example: Using 175m CPU of 250m request = 70% utilization\n",
    "```\n",
    "\n",
    "### Follow-up:\n",
    "- Why have both minReplicas and a LoadBalancer service?\n",
    "- What's the difference between Utilization and AverageValue metrics?\n",
    "- How would you scale based on custom metrics (e.g., request latency)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb971a",
   "metadata": {},
   "source": [
    "## Q2.3: What is a Pod Disruption Budget (PDB) and why do we use it?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**Pod Disruption Budget** = Guarantee minimum availability during \"voluntary\" disruptions\n",
    "\n",
    "### Types of Disruptions:\n",
    "\n",
    "**Voluntary Disruptions:**\n",
    "- Node maintenance (patching, upgrading)\n",
    "- Cluster autoscaling (removing underutilized nodes)\n",
    "- Manual kubectl drain\n",
    "- Pod evictions\n",
    "\n",
    "**Involuntary Disruptions:**\n",
    "- Hardware failure\n",
    "- Network partition\n",
    "- Power outage\n",
    "- Kernel panic\n",
    "- PDB does NOT protect against these\n",
    "\n",
    "### Our PDB Configuration:\n",
    "\n",
    "```yaml\n",
    "apiVersion: policy/v1\n",
    "kind: PodDisruptionBudget\n",
    "metadata:\n",
    "  name: ml-inference-pdb\n",
    "spec:\n",
    "  minAvailable: 1  # Always keep at least 1 pod running\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: ml-inference\n",
    "```\n",
    "\n",
    "### Scenario Without PDB:\n",
    "\n",
    "```\n",
    "Cluster has 2 nodes with 5 pods total:\n",
    "Node 1: 3 ml-inference pods\n",
    "Node 2: 2 ml-inference pods\n",
    "\n",
    "Node 1 needs security patch:\n",
    "1. Kubernetes drain Node 1 (evict all pods)\n",
    "2. 3 pods are killed immediately\n",
    "3. Only 2 pods remain on Node 2\n",
    "4. API is degraded (underprovisioned)\n",
    "5. HPA takes 30 seconds to notice and scale up\n",
    "6. Requests during this window: high latency/timeouts\n",
    "```\n",
    "\n",
    "### Scenario With PDB (minAvailable: 1):\n",
    "\n",
    "```\n",
    "Same scenario:\n",
    "Node 1: 3 ml-inference pods\n",
    "Node 2: 2 ml-inference pods\n",
    "\n",
    "Node 1 needs security patch:\n",
    "1. Kubernetes wants to drain Node 1\n",
    "2. PDB says: \"Keep at least 1 pod running\"\n",
    "3. Kubernetes evicts 2 pods (keeps 1 running)\n",
    "4. New pods scheduled on other nodes\n",
    "5. After evicted pods restart:\n",
    "   - Total 4 pods running during maintenance\n",
    "   - API remains responsive\n",
    "6. HPA may scale up if needed\n",
    "7. After Node 1 patched, pod returns\n",
    "```\n",
    "\n",
    "### PDB Strategies:\n",
    "\n",
    "**Strategy 1: minAvailable (absolute count)**\n",
    "```yaml\n",
    "minAvailable: 1  # Always 1+ pods\n",
    "# Good when you need guaranteed minimum replicas\n",
    "```\n",
    "\n",
    "**Strategy 2: minAvailable (percentage)**\n",
    "```yaml\n",
    "minAvailable: 50%  # Always 50%+ of replicas\n",
    "# With 2-10 replicas (HPA), scales dynamically\n",
    "# Min 2 replicas → minAvailable = 1\n",
    "# Max 10 replicas → minAvailable = 5\n",
    "```\n",
    "\n",
    "**Strategy 3: maxUnavailable (absolute count)**\n",
    "```yaml\n",
    "maxUnavailable: 1  # Max 1 pod can be disrupted\n",
    "# Opposite of minAvailable\n",
    "```\n",
    "\n",
    "### Why minAvailable: 1 for Our Setup:\n",
    "\n",
    "1. **HPA Range (2-10 replicas):** Even min 2 replicas is OK, but 1 provides safety\n",
    "2. **API Service:** Must remain responsive during maintenance\n",
    "3. **Graceful Degradation:** 1 pod is better than 0\n",
    "4. **Cost:** minAvailable: 1 doesn't require extra resources\n",
    "\n",
    "### Interaction with HPA:\n",
    "\n",
    "```yaml\n",
    "Deployment:\n",
    "  minReplicas: 2   # HPA keeps at least 2\n",
    "  maxReplicas: 10\n",
    "\n",
    "PDB:\n",
    "  minAvailable: 1  # During disruption, keep at least 1\n",
    "\n",
    "Result:\n",
    "- Normal state: 2-10 pods (HPA manages)\n",
    "- During disruption: At least 1 pod guaranteed\n",
    "```\n",
    "\n",
    "### Follow-up:\n",
    "- What happens if maxUnavailable exceeds minAvailable?\n",
    "- Can PDB prevent all disruptions?\n",
    "- How does PDB interact with node affinity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddbe82d",
   "metadata": {},
   "source": [
    "# 3. AWS & TERRAFORM\n",
    "\n",
    "## Q3.1: Explain IRSA (IAM Roles for Service Accounts) and why it's better than hardcoding credentials\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**IRSA** = Kubernetes ServiceAccount can assume AWS IAM roles\n",
    "\n",
    "### Traditional Approach (Bad):\n",
    "\n",
    "```yaml\n",
    "# Create AWS access keys\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: aws-credentials\n",
    "stringData:\n",
    "  AWS_ACCESS_KEY_ID: AKIAIOSFODNN7EXAMPLE\n",
    "  AWS_SECRET_ACCESS_KEY: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n",
    "\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: app\n",
    "        env:\n",
    "        - name: AWS_ACCESS_KEY_ID\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: aws-credentials\n",
    "              key: AWS_ACCESS_KEY_ID\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "- ✗ Long-lived credentials (never expire)\n",
    "- ✗ Credentials visible in pod environment\n",
    "- ✗ Hard to rotate (requires secret update + pod restart)\n",
    "- ✗ Difficult to audit (who has access?)\n",
    "- ✗ If pod compromised, attacker gets AWS keys\n",
    "- ✗ Cannot granularly control which pod has which role\n",
    "\n",
    "### IRSA Approach (Good):\n",
    "\n",
    "```yaml\n",
    "# 1. Kubernetes ServiceAccount\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: ml-inference-sa\n",
    "  annotations:\n",
    "    eks.amazonaws.com/role-arn: arn:aws:iam::802520734572:role/ml-inference-pod-role\n",
    "\n",
    "---\n",
    "# 2. Pod uses the ServiceAccount\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      serviceAccountName: ml-inference-sa\n",
    "      containers:\n",
    "      - name: ml-api\n",
    "        image: ml-inference-service:latest\n",
    "```\n",
    "\n",
    "### How IRSA Works (Behind the Scenes):\n",
    "\n",
    "```\n",
    "1. EKS Cluster has OIDC Provider\n",
    "   └─ URL: https://oidc.eks.ap-southeast-2.amazonaws.com/id/EXAMPLEID\n",
    "\n",
    "2. IAM Trust Relationship\n",
    "   └─ Role \"ml-inference-pod-role\" trusts the OIDC provider\n",
    "      for ServiceAccount \"ml-inference:ml-inference-sa\"\n",
    "\n",
    "3. When pod starts:\n",
    "   a) Kubernetes mounts ServiceAccount token into pod\n",
    "      └─ /var/run/secrets/eks.amazonaws.com/serviceaccount/token\n",
    "   \n",
    "   b) Pod makes STS AssumeRoleWithWebIdentity request\n",
    "      ├─ URL: https://sts.amazonaws.com\n",
    "      ├─ RoleArn: arn:aws:iam::802520734572:role/ml-inference-pod-role\n",
    "      └─ WebIdentityToken: (JWT from Kubernetes)\n",
    "   \n",
    "   c) AWS STS validates token (checks OIDC issuer)\n",
    "      └─ If valid: Issues temporary credentials\n",
    "   \n",
    "   d) Pod receives temporary AWS credentials\n",
    "      ├─ AWS_ROLE_ARN\n",
    "      └─ AWS_WEB_IDENTITY_TOKEN_FILE\n",
    "   \n",
    "   e) AWS SDK automatically uses these credentials\n",
    "      └─ No need to hardcode!\n",
    "\n",
    "4. Credentials auto-refresh\n",
    "   └─ AWS SDK refreshes before expiration (every ~1 hour)\n",
    "```\n",
    "\n",
    "### IRSA Benefits:\n",
    "\n",
    "✓ Short-lived credentials (temporary, ~1 hour)  \n",
    "✓ Auto-rotating (no manual rotation needed)  \n",
    "✓ Not stored in Kubernetes secrets  \n",
    "✓ Granular per-pod permissions  \n",
    "✓ Easy to audit (CloudTrail shows which SA made request)  \n",
    "✓ Pod doesn't have access to other pods' credentials  \n",
    "✓ Better security (stolen credential expires soon)  \n",
    "\n",
    "### Our Setup:\n",
    "\n",
    "**IAM Role (AWS side):**\n",
    "```json\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ecr:GetAuthorizationToken\",\n",
    "        \"ecr:BatchGetImage\",\n",
    "        \"logs:PutLogEvents\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    }\n",
    "  ]\n",
    ",\n",
    "Trust Relationship\":\n",
    "  {\n",
    "    \"Effect\": \"Allow\",\n",
    "    \"Principal\": {\n",
    "      \"Federated\": \"arn:aws:iam::802520734572:oidc-provider/oidc.eks.ap-southeast-2.amazonaws.com/id/EXAMPLEID\"\n",
    "    },\n",
    "    \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n",
    "    \"Condition\": {\n",
    "      \"StringEquals\": {\n",
    "        \"oidc.eks.ap-southeast-2.amazonaws.com/id/EXAMPLEID:sub\": \"system:serviceaccount:ml-inference:ml-inference-sa\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Kubernetes ServiceAccount:**\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: ml-inference-sa\n",
    "  namespace: ml-inference\n",
    "  annotations:\n",
    "    eks.amazonaws.com/role-arn: arn:aws:iam::802520734572:role/ml-inference-pod-role\n",
    "```\n",
    "\n",
    "### Follow-up:\n",
    "- How does IRSA differ from assuming a cross-account role?\n",
    "- What happens if the OIDC provider certificate expires?\n",
    "- Can you have multiple ServiceAccounts per pod?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47de7c67",
   "metadata": {},
   "source": [
    "## Q3.2: Explain Terraform state file and why remote state is important\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**Terraform State** = Database of all resources Terraform manages\n",
    "\n",
    "### What's in State File:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"version\": 4,\n",
    "  \"terraform_version\": \"1.5.0\",\n",
    "  \"resources\": [\n",
    "    {\n",
    "      \"type\": \"aws_eks_cluster\",\n",
    "      \"name\": \"main\",\n",
    "      \"instances\": [\n",
    "        {\n",
    "          \"attributes\": {\n",
    "            \"id\": \"ml-inference-prod-cluster\",\n",
    "            \"arn\": \"arn:aws:eks:ap-southeast-2:802520734572:cluster/ml-inference-prod-cluster\",\n",
    "            \"status\": \"ACTIVE\",\n",
    "            \"created_at\": \"2024-02-22T10:00:00Z\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Local State (Default) - Bad for Teams:\n",
    "\n",
    "```bash\n",
    "# terraform.tfstate stored locally\n",
    "project/\n",
    "├─ main.tf\n",
    "├─ variables.tf\n",
    "└─ terraform.tfstate  ← Only on your machine\n",
    "\n",
    "Problem:\n",
    "1. Alice runs terraform apply → state updated on Alice's machine\n",
    "2. Bob runs terraform plan → sees old state (resources missing)\n",
    "3. Bob runs terraform apply → creates duplicate resources!\n",
    "4. Conflicts, race conditions, resource drift\n",
    "```\n",
    "\n",
    "### Remote State (S3) - Good for Teams:\n",
    "\n",
    "```hcl\n",
    "terraform {\n",
    "  backend \"s3\" {\n",
    "    bucket         = \"my-terraform-state\"\n",
    "    key            = \"ml-inference/terraform.tfstate\"\n",
    "    region         = \"ap-southeast-2\"\n",
    "    encrypt        = true\n",
    "    dynamodb_table = \"terraform-locks\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Workflow:**\n",
    "```\n",
    "1. State stored in S3 (shared, remote)\n",
    "2. DynamoDB lock prevents simultaneous applies\n",
    "3. Alice runs terraform apply:\n",
    "   a) Terraform acquires DynamoDB lock\n",
    "   b) Reads state from S3\n",
    "   c) Makes changes\n",
    "   d) Writes state back to S3\n",
    "   e) Releases lock\n",
    "4. Bob waits for lock (cannot apply simultaneously)\n",
    "5. Bob runs terraform apply:\n",
    "   a) Sees latest state (from Alice)\n",
    "   b) Applies his changes\n",
    "   c) No conflicts!\n",
    "```\n",
    "\n",
    "### Our Recommendation (in production):\n",
    "\n",
    "```hcl\n",
    "terraform {\n",
    "  required_version = \">= 1.0\"\n",
    "  required_providers {\n",
    "    aws = {\n",
    "      source  = \"hashicorp/aws\"\n",
    "      version = \"~> 5.0\"\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  backend \"s3\" {\n",
    "    bucket         = \"company-terraform-state-802520734572\"  # Unique per account\n",
    "    key            = \"ml-inference/terraform.tfstate\"\n",
    "    region         = \"ap-southeast-2\"\n",
    "    encrypt        = true  # Encrypt at rest\n",
    "    dynamodb_table = \"terraform-locks\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### State File Security:\n",
    "\n",
    "**Sensitive data in state:**\n",
    "- Database passwords\n",
    "- API keys\n",
    "- Private key content\n",
    "\n",
    "**Protect state file:**\n",
    "1. Store in S3 with encryption\n",
    "   ```hcl\n",
    "   encrypt = true  # KMS encryption\n",
    "   ```\n",
    "\n",
    "2. Enable versioning\n",
    "   ```hcl\n",
    "   resource \"aws_s3_bucket_versioning\" \"terraform\" {\n",
    "     bucket = aws_s3_bucket.terraform.id\n",
    "     versioning_configuration {\n",
    "       status = \"Enabled\"\n",
    "     }\n",
    "   }\n",
    "   ```\n",
    "\n",
    "3. Block public access\n",
    "   ```hcl\n",
    "   resource \"aws_s3_bucket_public_access_block\" \"terraform\" {\n",
    "     bucket = aws_s3_bucket.terraform.id\n",
    "     block_public_acls       = true\n",
    "     block_public_policy     = true\n",
    "     ignore_public_acls      = true\n",
    "     restrict_public_buckets = true\n",
    "   }\n",
    "   ```\n",
    "\n",
    "4. Enable MFA delete\n",
    "   ```hcl\n",
    "   # Requires MFA to delete state versions\n",
    "   ```\n",
    "\n",
    "5. Restrict access\n",
    "   ```json\n",
    "   {\n",
    "     \"Version\": \"2012-10-17\",\n",
    "     \"Statement\": [{\n",
    "       \"Principal\": {\n",
    "         \"AWS\": \"arn:aws:iam::802520734572:role/TerraformRole\"\n",
    "       },\n",
    "       \"Effect\": \"Allow\",\n",
    "       \"Action\": \"s3:*\",\n",
    "       \"Resource\": [\n",
    "         \"arn:aws:s3:::terraform-state-bucket\",\n",
    "         \"arn:aws:s3:::terraform-state-bucket/*\"\n",
    "       ]\n",
    "     }]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "### Never Commit State to Git:\n",
    "\n",
    "```bash\n",
    "# .gitignore\n",
    "terraform.tfstate\n",
    "terraform.tfstate.*\n",
    ".terraform/\n",
    "*.tfvars\n",
    "```\n",
    "\n",
    "### Follow-up:\n",
    "- What's in the lock file (terraform.tflock)?\n",
    "- How do you recover from state corruption?\n",
    "- Can you migrate from local to remote state?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe73904",
   "metadata": {},
   "source": [
    "# 4. DEPLOYMENT & OPERATIONS\n",
    "\n",
    "## Q4.1: Explain the deployment process and what happens when you apply Kubernetes manifests\n",
    "\n",
    "### Answer:\n",
    "\n",
    "### Step-by-Step Deployment Process:\n",
    "\n",
    "```\n",
    "1. TERRAFORM PHASE (Infrastructure)\n",
    "   ├─ terraform init        → Initialize working directory\n",
    "   ├─ terraform plan        → Preview changes (review before applying!)\n",
    "   └─ terraform apply       → Create AWS resources:\n",
    "      ├─ VPC with 2 public, 2 private subnets\n",
    "      ├─ NAT Gateways for private subnet egress\n",
    "      ├─ EKS Control Plane (master nodes, AWS-managed)\n",
    "      ├─ EKS Node Group (worker nodes, EC2 instances)\n",
    "      ├─ Security Groups\n",
    "      ├─ IAM Roles and Policies\n",
    "      ├─ OIDC Provider\n",
    "      └─ CloudWatch Log Groups\n",
    "      \n",
    "      At this point:\n",
    "      - EKS cluster is running (empty, no pods yet)\n",
    "      - Nodes are ready to run containers\n",
    "      - Cluster is accessible via kubectl\n",
    "\n",
    "2. KUBECTL CONFIGURATION\n",
    "   └─ aws eks update-kubeconfig ...\n",
    "      └─ Adds cluster credentials to ~/.kube/config\n",
    "      └─ kubectl can now communicate with cluster\n",
    "\n",
    "3. KUBERNETES PHASE (Manifests)\n",
    "   ├─ kubectl apply -f 01-namespace-configmap.yaml\n",
    "   │  ├─ Creates namespace \"ml-inference\"\n",
    "   │  ├─ Creates ConfigMap with env variables\n",
    "   │  ├─ Creates Secret (if provided)\n",
    "   │  ├─ Creates ServiceAccount with IRSA annotation\n",
    "   │  ├─ Creates Deployment:\n",
    "   │  │  ├─ ReplicaSet (manages pod replicas)\n",
    "   │  │  ├─ Pods (2 initially, managed by HPA)\n",
    "   │  │  └─ Each pod creates container from ECR image\n",
    "   │  ├─ Creates Service (ClusterIP) for internal routing\n",
    "   │  ├─ Creates HPA (watches metrics, scales pods)\n",
    "   │  └─ Creates PDB (min 1 replica during disruption)\n",
    "   │\n",
    "   ├─ kubectl apply -f 02-ingress-network-policy.yaml\n",
    "   │  ├─ Creates Ingress (optional, for domain routing)\n",
    "   │  ├─ Creates NetworkPolicy (traffic restrictions)\n",
    "   │  ├─ Creates ResourceQuota (namespace limits)\n",
    "   │  └─ Creates LimitRange (per-container limits)\n",
    "   │\n",
    "   └─ kubectl apply -f 03-rbac-monitoring.yaml\n",
    "      ├─ Creates Role (permissions for SA)\n",
    "      ├─ Creates RoleBinding (assign role to SA)\n",
    "      └─ Creates ServiceMonitor (for Prometheus)\n",
    "```\n",
    "\n",
    "### What Happens When Pod Starts:\n",
    "\n",
    "```\n",
    "1. Kubernetes Scheduler picks a node\n",
    "   └─ Considers: resource requests, node affinity, pod affinity, taints/tolerations\n",
    "\n",
    "2. kubelet (node agent) receives pod spec\n",
    "   └─ Tells container runtime (Docker/containerd) to create container\n",
    "\n",
    "3. Container Runtime pulls image\n",
    "   ├─ From ECR: 802520734572.dkr.ecr.ap-southeast-2.amazonaws.com/ml-inference-service:latest\n",
    "   ├─ Uses ECR credentials (via IRSA)\n",
    "   └─ Caches image on node\n",
    "\n",
    "4. Container starts\n",
    "   ├─ Mounts volumes (ConfigMap, ServiceAccount token)\n",
    "   ├─ Sets environment variables\n",
    "   ├─ Runs startup command: python src/app.py\n",
    "   └─ Sets resource limits (CPU: 250m-1000m, Memory: 512Mi-1Gi)\n",
    "\n",
    "5. Startup Probe begins\n",
    "   ├─ Every 10 seconds, checks GET /health\n",
    "   ├─ If fails: increments failure counter\n",
    "   ├─ After 3 failures (30 seconds): pod is considered failed\n",
    "   └─ After success: switches to liveness/readiness probes\n",
    "\n",
    "6. Pod Initialization (if successful startup probe)\n",
    "   ├─ Container fully running\n",
    "   ├─ Liveness probe begins (restart if unhealthy)\n",
    "   ├─ Readiness probe begins (remove from LB if unhealthy)\n",
    "   └─ Pod status: Running\n",
    "\n",
    "7. Network Connectivity\n",
    "   ├─ Pod gets IP address (from Calico/Flannel)\n",
    "   ├─ Service gets endpoints (list of pod IPs)\n",
    "   ├─ LoadBalancer gets targets (pod IPs)\n",
    "   └─ Traffic can flow\n",
    "\n",
    "8. HPA Monitoring\n",
    "   ├─ Metrics Server collects CPU/memory every 15 seconds\n",
    "   ├─ HPA checks metrics every 30 seconds\n",
    "   ├─ If utilization > 70% (CPU) or > 80% (memory): scale up\n",
    "   └─ New pods follow same startup process\n",
    "```\n",
    "\n",
    "### Timeline Example:\n",
    "\n",
    "```\n",
    "t=0s      kubectl apply -f manifests\n",
    "          └─ API server receives manifests\n",
    "\n",
    "t=1s      Deployment created\n",
    "          └─ ReplicaSet created\n",
    "          └─ 2 Pods created (minReplicas from HPA)\n",
    "\n",
    "t=2s      Scheduler assigns pods to nodes\n",
    "          └─ Pod 1 → Node 1\n",
    "          └─ Pod 2 → Node 2 (anti-affinity)\n",
    "\n",
    "t=5s      kubelet pulls image\n",
    "          └─ ~2-3 seconds for 500MB image\n",
    "\n",
    "t=8s      Container started\n",
    "          └─ FastAPI server initializing\n",
    "          └─ Loading ML model (~1 second)\n",
    "\n",
    "t=10s     Startup probe: 1st check → GET /health → ✓ Success\n",
    "t=20s     Startup probe: 2nd check → ✓ Success\n",
    "t=30s     Startup probe: 3rd check → ✓ Success\n",
    "          └─ Switched to liveness/readiness\n",
    "\n",
    "t=40s     Readiness probe: 1st check → ✓ Success\n",
    "t=45s     Readiness probe: 2nd check → ✓ Success\n",
    "          └─ Pod added to Service endpoints\n",
    "          └─ Pod added to LoadBalancer targets\n",
    "\n",
    "t=50s     Traffic begins flowing to pod\n",
    "          └─ First inference requests incoming\n",
    "\n",
    "t=60s     Service fully ready with both pods\n",
    "          └─ API responding to health checks\n",
    "          └─ Ready for production traffic\n",
    "```\n",
    "\n",
    "### Kubectl apply vs delete vs patch:\n",
    "\n",
    "```bash\n",
    "# Apply: Idempotent (safe to run multiple times)\n",
    "kubectl apply -f manifests.yaml\n",
    "# → Creates if doesn't exist\n",
    "# → Updates if exists\n",
    "# → Stores applied configuration in annotation\n",
    "\n",
    "# Delete: Removes resource and pods (with grace period)\n",
    "kubectl delete -f manifests.yaml\n",
    "# → Pods get 30 seconds (terminationGracePeriodSeconds) to shutdown\n",
    "# → After 30s: forcefully killed\n",
    "# → Resource deleted from cluster\n",
    "\n",
    "# Patch: Surgical update (changes specific fields)\n",
    "kubectl patch deployment ml-inference-api -n ml-inference \\\n",
    "  -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"ml-api\",\"image\":\"ml-inference-service:v2\"}]}}}}'\n",
    "# → Updates just the image, keeps other config\n",
    "# → Triggers rolling update\n",
    "```\n",
    "\n",
    "### Follow-up:\n",
    "- What's the difference between Recreate and RollingUpdate deployment strategy?\n",
    "- How does graceful shutdown work (SIGTERM vs SIGKILL)?\n",
    "- What happens if image pull fails?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd117c",
   "metadata": {},
   "source": [
    "# 5. ML SPECIFIC QUESTIONS\n",
    "\n",
    "## Q5.1: How would you handle model versioning in production?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**Model Versioning Strategies:**\n",
    "\n",
    "### Strategy 1: Docker Image Tags\n",
    "```bash\n",
    "# Build with model version\n",
    "docker build -t ml-inference-service:1.0 .\n",
    "docker build -t ml-inference-service:1.1 .\n",
    "docker build -t ml-inference-service:2.0 .\n",
    "\n",
    "# Deploy specific version\n",
    "kubectl set image deployment/ml-inference-api \\\n",
    "  ml-api=ml-inference-service:2.0 \\\n",
    "  -n ml-inference\n",
    "\n",
    "# Automatic rolling update (old pods gradually replaced with new)\n",
    "```\n",
    "\n",
    "### Strategy 2: Model Registry\n",
    "```python\n",
    "# MLflow, DVC, or custom registry\n",
    "import mlflow\n",
    "\n",
    "# Register model\n",
    "mlflow.sklearn.log_model(model, \"ml-inference-service\")\n",
    "# → Creates version 1, 2, 3, etc.\n",
    "\n",
    "# In production: Load specific version\n",
    "model = mlflow.sklearn.load_model(\"models:/ml-inference-service/1\")\n",
    "```\n",
    "\n",
    "### Strategy 3: Canary Deployment\n",
    "```yaml\n",
    "# Gradually roll out new model version\n",
    "# 10% traffic to v2, 90% to v1\n",
    "# Monitor metrics, then gradually increase\n",
    "\n",
    "# Phase 1: 10% canary\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: VirtualService\n",
    "metadata:\n",
    "  name: ml-inference\n",
    "spec:\n",
    "  hosts:\n",
    "  - ml-inference\n",
    "  http:\n",
    "  - match:\n",
    "    - headers:\n",
    "        user-agent:\n",
    "          regex: \".*canary.*\"\n",
    "    route:\n",
    "    - destination:\n",
    "        host: ml-inference\n",
    "        subset: v2  # New model\n",
    "  - route:  # 90% default traffic\n",
    "    - destination:\n",
    "        host: ml-inference\n",
    "        subset: v1  # Old model\n",
    "```\n",
    "\n",
    "### Our Recommendation:\n",
    "```yaml\n",
    "# Store model version in ConfigMap\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: ml-config\n",
    "data:\n",
    "  MODEL_VERSION: \"2.0.1\"\n",
    "  MODEL_S3_PATH: \"s3://models/ml-inference/v2.0.1/model.pkl\"\n",
    "\n",
    "---\n",
    "# Pod reads this and loads correct model\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: ml-api\n",
    "        env:\n",
    "        - name: MODEL_VERSION\n",
    "          valueFrom:\n",
    "            configMapKeyRef:\n",
    "              name: ml-config\n",
    "              key: MODEL_VERSION\n",
    "        - name: MODEL_S3_PATH\n",
    "          valueFrom:\n",
    "            configMapKeyRef:\n",
    "              name: ml-config\n",
    "              key: MODEL_S3_PATH\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- ✓ Update model without rebuilding Docker image\n",
    "- ✓ Quick rollback (just change ConfigMap)\n",
    "- ✓ A/B testing possible (route to different versions)\n",
    "- ✓ No deployment downtime\n",
    "\n",
    "### Follow-up:\n",
    "- How do you handle incompatible model versions?\n",
    "- What metrics would you monitor for a new model?\n",
    "- How would you implement shadow mode testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87b0c0f",
   "metadata": {},
   "source": [
    "## Q5.2: Explain the flow from prediction request to response\n",
    "\n",
    "### Answer:\n",
    "\n",
    "```\n",
    "1. CLIENT REQUEST\n",
    "   └─ curl -X POST http://<LB-URL>/predict-single \\\n",
    "      -H 'Content-Type: application/json' \\\n",
    "      -d '{\"features\": [1,2,3,...,20]}'\n",
    "\n",
    "2. AWS NLB (Network Load Balancer)\n",
    "   ├─ Receives request on port 80\n",
    "   ├─ Performs Layer 4 routing (TCP)\n",
    "   ├─ Selects backend pod (IP: 10.0.11.45, port: 5000)\n",
    "   └─ Forwards request\n",
    "\n",
    "3. KUBERNETES NETWORK\n",
    "   ├─ Request hits pod network interface (eth0)\n",
    "   ├─ iptables rules (Calico/Flannel) route to container\n",
    "   └─ Request forwarded to FastAPI server\n",
    "\n",
    "4. FASTAPI ENDPOINT (/predict-single)\n",
    "   ├─ Request: POST /predict-single\n",
    "   ├─ Body: {\"features\": [1,2,3,...,20]}\n",
    "   │\n",
    "   ├─ Pydantic validation\n",
    "   │  └─ Validates 20 features provided (correct type, range)\n",
    "   │\n",
    "   ├─ Feature preprocessing\n",
    "   │  └─ Reshape to 2D array: (1, 20)\n",
    "   │\n",
    "   └─ Model inference\n",
    "      └─ Call model.predict(features)\n",
    "\n",
    "5. MODEL PREDICTION (SKLearn Random Forest)\n",
    "   ├─ Load preprocessor (StandardScaler)\n",
    "   │  └─ Standardize features: (value - mean) / std\n",
    "   │\n",
    "   ├─ Load model (100 decision trees)\n",
    "   │  └─ Each tree votes on class\n",
    "   │\n",
    "   ├─ Prediction phase\n",
    "   │  ├─ Each tree: pass through splits, reach leaf\n",
    "   │  ├─ Leaf predicts a class (0, 1, or 2)\n",
    "   │  ├─ Aggregate 100 tree predictions (majority vote)\n",
    "   │  └─ Return final class + probabilities\n",
    "   │\n",
    "   └─ Return\n",
    "      ├─ Prediction: 1 (class)\n",
    "      └─ Probabilities: [0.05, 0.85, 0.10]\n",
    "\n",
    "6. API RESPONSE CONSTRUCTION\n",
    "   ├─ Class 0: 5% confidence\n",
    "   ├─ Class 1: 85% confidence (highest)\n",
    "   ├─ Class 2: 10% confidence\n",
    "   │\n",
    "   └─ Serialize to JSON\n",
    "      {\n",
    "        \"prediction\": 1,\n",
    "        \"confidence\": 0.85,\n",
    "        \"probabilities\": [0.05, 0.85, 0.10]\n",
    "      }\n",
    "\n",
    "7. FASTAPI SERIALIZATION\n",
    "   ├─ Convert numpy arrays to JSON\n",
    "   ├─ Add HTTP headers\n",
    "   │  ├─ Content-Type: application/json\n",
    "   │  ├─ Content-Length: 87\n",
    "   │  └─ ...\n",
    "   └─ Response code: 200 OK\n",
    "\n",
    "8. NETWORK RESPONSE\n",
    "   ├─ Pod sends response to client\n",
    "   ├─ NLB forwards response (Layer 4 NAT)\n",
    "   └─ Client receives JSON\n",
    "\n",
    "9. CLIENT PROCESSING\n",
    "   └─ Parse JSON response\n",
    "   └─ Use prediction: class 1\n",
    "```\n",
    "\n",
    "### Latency Breakdown (typical numbers):\n",
    "\n",
    "```\n",
    "Network latency         1 ms   (client to NLB)\n",
    "NLB routing             1 ms   (L4 load balancer)\n",
    "Kubernetes network      2 ms   (iptables, overlay network)\n",
    "FastAPI parsing/validation 1 ms   (request deserialization)\n",
    "Feature preprocessing   0.1 ms  (standardscaler on 20 features)\n",
    "Model inference         2-5 ms  (random forest 100 trees)\n",
    "Response serialization  1 ms   (JSON encoding)\n",
    "Network latency         1 ms   (response to client)\n",
    "────────────────────────────────\n",
    "Total                   ~10 ms\n",
    "```\n",
    "\n",
    "### Optimization Strategies:\n",
    "\n",
    "**1. Batch Predictions**\n",
    "```python\n",
    "# Instead of 1 prediction at a time\n",
    "# Send 100 predictions in one request\n",
    "# Amortizes overhead\n",
    "@app.post(\"/predict\")\n",
    "async def predict_batch(request: PredictRequest):\n",
    "    # request.features shape: (100, 20)\n",
    "    predictions = model.predict(request.features)\n",
    "    # Return 100 predictions at once\n",
    "```\n",
    "\n",
    "**2. Model Quantization**\n",
    "```python\n",
    "# Reduce model size\n",
    "# int8 instead of float32 = 4x smaller\n",
    "# Inference 10-20% faster\n",
    "from skl2onnx import convert_sklearn\n",
    "import onnx\n",
    "\n",
    "onnx_model = convert_sklearn(model)\n",
    "# Export to ONNX Runtime (faster inference)\n",
    "```\n",
    "\n",
    "**3. Caching**\n",
    "```python\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def predict(features_tuple):\n",
    "    # Cache predictions for identical inputs\n",
    "    # Repeated predictions return cached result (0 ms)\n",
    "```\n",
    "\n",
    "### Follow-up:\n",
    "- How would you implement request batching?\n",
    "- What's the difference between latency and throughput?\n",
    "- How would you profile model inference time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b5ab4f",
   "metadata": {},
   "source": [
    "# 6. TROUBLESHOOTING & PRODUCTION ISSUES\n",
    "\n",
    "## Q6.1: Pod is not starting. How would you debug?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "### Step 1: Check Pod Status\n",
    "```bash\n",
    "kubectl get pods -n ml-inference\n",
    "# Output:\n",
    "# NAME                      READY STATUS             AGE\n",
    "# ml-inference-api-xyz      0/1   ImagePullBackOff   2m\n",
    "# ml-inference-api-abc      0/1   Pending             3m\n",
    "# ml-inference-api-def      0/1   CrashLoopBackOff   1m\n",
    "```\n",
    "\n",
    "**Status meanings:**\n",
    "- **Pending:** Waiting to be scheduled (not enough resources)\n",
    "- **ImagePullBackOff:** Cannot pull image from registry\n",
    "- **CrashLoopBackOff:** Container starts but crashes immediately\n",
    "- **Running:** Container running but may not be ready\n",
    "\n",
    "### Step 2: Describe Pod (detailed info)\n",
    "```bash\n",
    "kubectl describe pod <pod-name> -n ml-inference\n",
    "# Shows:\n",
    "# - Events (last 10 events)\n",
    "# - Image pull status\n",
    "# - Probe failure reasons\n",
    "# - Node assignment\n",
    "# - Container status\n",
    "```\n",
    "\n",
    "**Common Event Messages:**\n",
    "```\n",
    "Failed to pull image \"...\": rpc error: code = Unknown\n",
    "  → ECR authentication failed\n",
    "  → Check IRSA role permissions\n",
    "\n",
    "Insufficient cpu/memory\n",
    "  → Node doesn't have enough resources\n",
    "  → Need more nodes (cluster autoscale)\n",
    "\n",
    "Liveness probe failed\n",
    "  → Container is alive but /health endpoint failed\n",
    "  → App crashed or hanging\n",
    "```\n",
    "\n",
    "### Step 3: Check Logs\n",
    "```bash\n",
    "# Current logs\n",
    "kubectl logs <pod-name> -n ml-inference\n",
    "\n",
    "# Previous logs (if pod crashed)\n",
    "kubectl logs <pod-name> -n ml-inference --previous\n",
    "\n",
    "# Stream logs (follow)\n",
    "kubectl logs -f <pod-name> -n ml-inference\n",
    "\n",
    "# Logs from specific container (if multiple)\n",
    "kubectl logs <pod-name> -c ml-api -n ml-inference\n",
    "```\n",
    "\n",
    "**Common Log Messages:**\n",
    "```\n",
    "ModuleNotFoundError: No module named 'sklearn'\n",
    "  → Dependency missing in Docker image\n",
    "  → Update requirements.txt and rebuild image\n",
    "\n",
    "FileNotFoundError: [Errno 2] No such file or directory: 'models/model.pkl'\n",
    "  → Model files not in Docker image\n",
    "  → Check COPY models/ in Dockerfile\n",
    "\n",
    "PermissionError: /var/log/app.log\n",
    "  → Pod running as non-root user without write permission\n",
    "  → Use emptyDir volume for logs\n",
    "\n",
    "Address already in use: port 5000\n",
    "  → Two processes trying to use same port\n",
    "  → Check if app is running twice\n",
    "```\n",
    "\n",
    "### Step 4: Probe Failures\n",
    "```bash\n",
    "# Test endpoint manually\n",
    "kubectl port-forward <pod-name> 5000:5000 -n ml-inference\n",
    "# In another terminal:\n",
    "curl http://localhost:5000/health\n",
    "\n",
    "# Or exec into pod\n",
    "kubectl exec -it <pod-name> -n ml-inference -- /bin/bash\n",
    "$ curl http://localhost:5000/health\n",
    "$ python -c \"import sklearn; print(sklearn.__version__)\"\n",
    "```\n",
    "\n",
    "### Step 5: Check Resource Limits\n",
    "```bash\n",
    "kubectl describe node <node-name>\n",
    "# Shows:\n",
    "# - Allocatable resources\n",
    "# - Current usage\n",
    "# - Pods running on node\n",
    "\n",
    "# Check if pod is being OOMKilled\n",
    "kubectl get pod <pod-name> -n ml-inference -o jsonpath='{.status.containerStatuses[0].lastState.terminated.reason}'\n",
    "# Output: OOMKilled\n",
    "# → Pod using more memory than limit (512Mi)\n",
    "# → Increase memory limit or reduce model size\n",
    "```\n",
    "\n",
    "### Step 6: Events from Cluster\n",
    "```bash\n",
    "# View all events in namespace\n",
    "kubectl get events -n ml-inference --sort-by='.lastTimestamp'\n",
    "\n",
    "# Watch events in real-time\n",
    "kubectl get events -n ml-inference --watch\n",
    "```\n",
    "\n",
    "### Debugging Checklist:\n",
    "\n",
    "```\n",
    "1. Pod status:\n",
    "   ☐ ImagePullBackOff → Check image name, ECR credentials\n",
    "   ☐ Pending → Check resources (kubectl top nodes)\n",
    "   ☐ CrashLoopBackOff → Check logs (kubectl logs --previous)\n",
    "\n",
    "2. Image issues:\n",
    "   ☐ Image exists in ECR? (aws ecr describe-images ...)\n",
    "   ☐ Tag is correct? (latest, v1.0, etc.)\n",
    "   ☐ IRSA role has ECR permissions? (aws iam get-role-policy ...)\n",
    "\n",
    "3. App issues:\n",
    "   ☐ Dependencies installed? (pip list in image)\n",
    "   ☐ Config files present? (ls -la /app/)\n",
    "   ☐ Permissions correct? (ls -la <file>, whoami)\n",
    "\n",
    "4. Probes:\n",
    "   ☐ /health endpoint works? (curl localhost:5000/health)\n",
    "   ☐ Port is correct? (5000, not 8000)\n",
    "   ☐ Timeout too short? (increase initialDelaySeconds)\n",
    "\n",
    "5. Resources:\n",
    "   ☐ Enough CPU? (kubectl top nodes)\n",
    "   ☐ Enough memory? (check for OOMKilled)\n",
    "   ☐ Disk space? (df -h inside pod)\n",
    "```\n",
    "\n",
    "### Follow-up:\n",
    "- How would you debug a pod that's running but not responding to traffic?\n",
    "- What metrics would you monitor to detect issues early?\n",
    "- How would you implement better logging for debugging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ebadf",
   "metadata": {},
   "source": [
    "# 7. ARCHITECTURE & DESIGN DECISIONS\n",
    "\n",
    "## Q7.1: Why did you choose EKS over other options (EC2, Fargate, App Runner)?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "### Comparison Table:\n",
    "\n",
    "| Aspect | EC2 (Manual) | ECS Fargate | App Runner | EKS |\n",
    "|--------|------------|-----------|-----------|-----|\n",
    "| **Control** | Full | Medium | Low | Full |\n",
    "| **Learning Curve** | High | Medium | Low | Very High |\n",
    "| **Flexibility** | Maximum | Good | Limited | Maximum |\n",
    "| **Scaling** | Manual/ASG | Automatic | Automatic | Automatic (HPA) |\n",
    "| **Cost** | Low (self-manage) | Medium | Medium-High | Medium |\n",
    "| **Multi-pod per node** | ✓ (if deployed) | ✓ | ✗ (one app per) | ✓ |\n",
    "| **Pod networking** | Manual | AWS ENI | Simple | CNI plugins |\n",
    "| **Use Case** | Legacy, simple | Container apps | Simple web apps | Complex microservices |\n",
    "| **Our Choice** | ✗ | ~ | ✗ | ✓ |\n",
    "\n",
    "### Why EKS for Our Project:\n",
    "\n",
    "**1. Future Scalability**\n",
    "```\n",
    "Today: Single ML API\n",
    "Future: Multiple services\n",
    "  ├─ ML API v1 (current model)\n",
    "  ├─ ML API v2 (new model, A/B testing)\n",
    "  ├─ Feature service (feature engineering)\n",
    "  ├─ Monitoring service (Prometheus)\n",
    "  ├─ Logging service (Fluentd)\n",
    "  └─ Admin dashboard\n",
    "\n",
    "EKS can manage all these efficiently.\n",
    "Fargate would create N independent services (costly).\n",
    "```\n",
    "\n",
    "**2. Advanced Deployment Patterns**\n",
    "```yaml\n",
    "# Canary deployments\n",
    "# 10% traffic → new model, 90% → old model\n",
    "# Monitor metrics, gradually increase\n",
    "# → Only possible with orchestration\n",
    "\n",
    "# Rolling updates\n",
    "# Graceful pod termination (30s shutdown period)\n",
    "# → Good for long-running jobs\n",
    "\n",
    "# Resource isolation\n",
    "# Multiple apps on same node\n",
    "# Each gets guaranteed resources\n",
    "```\n",
    "\n",
    "**3. Team Skill Transfer**\n",
    "```\n",
    "Kubernetes is industry standard.\n",
    "Skills transfer to other companies/projects.\n",
    "ECS is AWS-specific (lock-in).\n",
    "```\n",
    "\n",
    "**4. Ecosystem & Integrations**\n",
    "```\n",
    "Kubernetes has massive ecosystem:\n",
    "  - Prometheus (monitoring)\n",
    "  - Istio (service mesh)\n",
    "  - Helm (package manager)\n",
    "  - Operators (custom logic)\n",
    "  - KNative (serverless)\n",
    "\n",
    "ECS is limited to AWS services.\n",
    "```\n",
    "\n",
    "**5. Multi-cloud Capability**\n",
    "```\n",
    "Kubernetes manifests work on:\n",
    "  - AWS EKS\n",
    "  - Google GKE\n",
    "  - Azure AKS\n",
    "  - On-premises (minikube, kubeadm)\n",
    "  - Bare metal\n",
    "\n",
    "Lock-in to single cloud is avoided.\n",
    "```\n",
    "\n",
    "### Why Not Fargate (Serverless Containers)?\n",
    "\n",
    "**Fargate Advantages:**\n",
    "- No node management\n",
    "- Simpler to start\n",
    "- Pay per container (fine-grained billing)\n",
    "\n",
    "**Fargate Disadvantages:**\n",
    "- Limited control (can't customize networking, kernel params)\n",
    "- Cold starts (slight latency on new container creation)\n",
    "- Expensive for sustained workloads (ECS Fargate ~$0.29/vCPU/hour vs EC2 ~$0.03/hour)\n",
    "- Cannot run multiple pods per task\n",
    "- Limited networking (no host networking)\n",
    "- Vendor lock-in (AWS only)\n",
    "\n",
    "**When Fargate makes sense:**\n",
    "- Simple, bursty workloads\n",
    "- Unpredictable traffic patterns\n",
    "- Small teams (less ops overhead)\n",
    "- Short-lived jobs\n",
    "\n",
    "### Why Not App Runner?\n",
    "\n",
    "**App Runner is for simple applications:**\n",
    "```\n",
    "  ✓ Connect GitHub repo\n",
    "  ✓ Auto-deploy on commit\n",
    "  ✓ TLS/SSL automatic\n",
    "  ✗ No pod management\n",
    "  ✗ No multi-container\n",
    "  ✗ Limited customization\n",
    "```\n",
    "\n",
    "### Why Not Manual EC2?\n",
    "\n",
    "**Manual EC2 issues:**\n",
    "```\n",
    "  ✗ No auto-scaling (ops calls scaling API)\n",
    "  ✗ No auto-restart (manual SSH to restart)\n",
    "  ✗ No rolling updates (downtime during deploy)\n",
    "  ✗ Ops overhead (patching, security updates)\n",
    "  ✗ Resource fragmentation (hard to pack apps efficiently)\n",
    "  ✗ No health checks (monitor manually)\n",
    "```\n",
    "\n",
    "### Our Final Decision:\n",
    "\n",
    "```\n",
    "EKS because:\n",
    "\n",
    "✓ Industry standard (Kubernetes)\n",
    "✓ Scalable to many services\n",
    "✓ Advanced deployment patterns (canary, shadow, A/B testing)\n",
    "✓ Multi-cloud flexibility\n",
    "✓ Rich ecosystem (monitoring, logging, service mesh)\n",
    "✓ Team skill development\n",
    "✓ Proven for ML model serving (Netflix, Uber, etc.)\n",
    "\n",
    "Trade-off:\n",
    "  - Higher learning curve\n",
    "  - More initial complexity\n",
    "  - But pays off as system grows\n",
    "```\n",
    "\n",
    "### Follow-up:\n",
    "- At what point would you reconsider (maybe move to Fargate)?\n",
    "- How would cost change if we used Fargate instead?\n",
    "- What other deployment targets are viable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b964e5c",
   "metadata": {},
   "source": [
    "# 8. ADVANCED TOPICS & OPEN-ENDED QUESTIONS\n",
    "\n",
    "## Q8.1: How would you implement zero-downtime deployments?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "### Zero-Downtime Deployment Process:\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: ml-inference-api\n",
    "spec:\n",
    "  # Key settings for zero-downtime\n",
    "  strategy:\n",
    "    type: RollingUpdate\n",
    "    rollingUpdate:\n",
    "      maxSurge: 1        # 1 extra pod allowed (total 3 during update)\n",
    "      maxUnavailable: 0  # 0 pods allowed down (key!)\n",
    "  \n",
    "  template:\n",
    "    spec:\n",
    "      # Graceful shutdown\n",
    "      terminationGracePeriodSeconds: 30\n",
    "      \n",
    "      containers:\n",
    "      - name: ml-api\n",
    "        lifecycle:\n",
    "          preStop:\n",
    "            exec:\n",
    "              command: [\"/bin/sh\", \"-c\", \"sleep 5\"]\n",
    "              # Give load balancer 5 sec to drain connections\n",
    "        \n",
    "        # Health checks\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 5000\n",
    "          initialDelaySeconds: 10\n",
    "          periodSeconds: 5  # Check often\n",
    "          failureThreshold: 2\n",
    "        \n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 5000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "```\n",
    "\n",
    "### Timeline of Zero-Downtime Deployment:\n",
    "\n",
    "```\n",
    "t=0s      kubectl set image deployment/ml-inference-api ml-api=ml-inference-service:v2\n",
    "          → Triggers rolling update\n",
    "\n",
    "t=1s      Kubernetes creates new pod (v2)\n",
    "          Current state:\n",
    "            - Pod 1 (v1) ← Receiving traffic\n",
    "            - Pod 2 (v1) ← Receiving traffic\n",
    "            - Pod 3 (v2) ← Starting (not ready yet)\n",
    "\n",
    "t=10s     Pod 3 readiness check 1: PASS\n",
    "t=15s     Pod 3 readiness check 2: PASS\n",
    "t=15s     Pod 3 added to LoadBalancer\n",
    "          Current state:\n",
    "            - Pod 1 (v1) ← Receiving traffic\n",
    "            - Pod 2 (v1) ← Receiving traffic\n",
    "            - Pod 3 (v2) ← Now receiving traffic\n",
    "\n",
    "t=16s     Kubernetes sends SIGTERM to Pod 1 (v1)\n",
    "          → Signals graceful shutdown\n",
    "          Current state:\n",
    "            - Pod 1 (v1) ← Shutting down (no new traffic)\n",
    "            - Pod 2 (v1) ← Receiving traffic\n",
    "            - Pod 3 (v2) ← Receiving traffic\n",
    "\n",
    "t=16-21s  preStop hook executes (sleep 5)\n",
    "          → LoadBalancer drains existing connections\n",
    "          → In-flight requests complete\n",
    "\n",
    "t=21s     Pod 1 terminates (process shuts down cleanly)\n",
    "          New pod created for Pod 2 replacement\n",
    "          Current state:\n",
    "            - Pod 2 (v1) ← Receiving traffic\n",
    "            - Pod 3 (v2) ← Receiving traffic\n",
    "            - Pod 4 (v2) ← Starting\n",
    "\n",
    "t=31s     Pod 4 becomes ready\n",
    "          LoadBalancer starts routing to Pod 4\n",
    "\n",
    "t=32s     SIGTERM sent to Pod 2 (v1)\n",
    "          Current state:\n",
    "            - Pod 2 (v1) ← Shutting down\n",
    "            - Pod 3 (v2) ← Receiving traffic\n",
    "            - Pod 4 (v2) ← Receiving traffic\n",
    "\n",
    "t=46s     Pod 2 terminates\n",
    "          Update complete! All pods running v2\n",
    "          Current state:\n",
    "            - Pod 3 (v2) ← Receiving traffic\n",
    "            - Pod 4 (v2) ← Receiving traffic\n",
    "\n",
    "THROUGHOUT:\n",
    "- At least 2 pods always ready (minReplicas: 2)\n",
    "- LoadBalancer always has healthy targets\n",
    "- Traffic never interrupted\n",
    "- No 502/503 errors\n",
    "```\n",
    "\n",
    "### Critical Settings for Zero-Downtime:\n",
    "\n",
    "```yaml\n",
    "# 1. maxUnavailable: 0\n",
    "   → Never remove more pods than available\n",
    "   → Ensures continuous availability\n",
    "\n",
    "# 2. maxSurge: 1 (or percentage)\n",
    "   → Allows temporary over-provisioning\n",
    "   → New pod starts before old pod killed\n",
    "\n",
    "# 3. terminationGracePeriodSeconds: 30\n",
    "   → Give app 30 seconds to shutdown gracefully\n",
    "   → Listen for SIGTERM, finish requests, close connections\n",
    "\n",
    "# 4. preStop hook\n",
    "   → Execute before SIGTERM\n",
    "   → Sleep to allow connection draining\n",
    "   → Wait for load balancer to remove from targets\n",
    "\n",
    "# 5. readinessProbe\n",
    "   → Only add pod to LB after passing\n",
    "   → Fast detection of issues\n",
    "   → Remove unhealthy pods from rotation\n",
    "```\n",
    "\n",
    "### Application Code for Graceful Shutdown:\n",
    "\n",
    "```python\n",
    "import signal\n",
    "import asyncio\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "shutdown_event = asyncio.Event()\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup():\n",
    "    # Load model, initialize connections\n",
    "    global model\n",
    "    model = load_model()\n",
    "\n",
    "@app.on_event(\"shutdown\")\n",
    "async def shutdown():\n",
    "    # Clean up resources\n",
    "    await close_database_connections()\n",
    "    await close_cache_connections()\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    print(\"SIGTERM received, gracefully shutting down...\")\n",
    "    shutdown_event.set()\n",
    "\n",
    "signal.signal(signal.SIGTERM, signal_handler)\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    if shutdown_event.is_set():\n",
    "        return {\"status\": \"shutting_down\"}, 503\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(request: PredictRequest):\n",
    "    if shutdown_event.is_set():\n",
    "        return {\"error\": \"Service shutting down\"}, 503\n",
    "    \n",
    "    # Process prediction\n",
    "    result = model.predict(request.features)\n",
    "    return {\"prediction\": result}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=5000)\n",
    "```\n",
    "\n",
    "### Testing Zero-Downtime:\n",
    "\n",
    "```bash\n",
    "# Terminal 1: Deploy new version\n",
    "kubectl set image deployment/ml-inference-api \\\n",
    "  ml-api=ml-inference-service:v2\n",
    "\n",
    "# Terminal 2: Monitor traffic\n",
    "while true; do\n",
    "  curl -s http://<LB-URL>/health | jq .\n",
    "  sleep 0.1\n",
    "done\n",
    "\n",
    "# Result: All responses are 200 OK (no errors)\n",
    "```\n",
    "\n",
    "### Follow-up:\n",
    "- What's the difference between Recreate and RollingUpdate strategy?\n",
    "- How would you handle database schema migrations?\n",
    "- How would you rollback a bad deployment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bb1292",
   "metadata": {},
   "source": [
    "## Q8.2: Discuss potential issues and how you'd handle them\n",
    "\n",
    "### Answer:\n",
    "\n",
    "### Common Production Issues:\n",
    "\n",
    "**1. Memory Leak in Model**\n",
    "```\n",
    "Problem:\n",
    "  - Pod memory gradually increases (not reclaimed)\n",
    "  - After days: Pod OOMKilled\n",
    "  - Service degrades\n",
    "\n",
    "Detection:\n",
    "  - Monitor memory trends (CloudWatch)\n",
    "  - Prometheus metrics: container_memory_usage_bytes\n",
    "\n",
    "Solution:\n",
    "  1. Identify leak (memory profiler in Python)\n",
    "  2. Temporary: Reduce memory limit to force restart\n",
    "  3. Permanent: Fix code, deploy new version\n",
    "  \n",
    "  apiVersion: apps/v1\n",
    "  kind: Deployment\n",
    "  spec:\n",
    "    template:\n",
    "      spec:\n",
    "        containers:\n",
    "        - name: ml-api\n",
    "          resources:\n",
    "            limits:\n",
    "              memory: \"512Mi\"  # Restart after 512MB\n",
    "```\n",
    "\n",
    "**2. Model Inference Slow Under Load**\n",
    "```\n",
    "Problem:\n",
    "  - API latency increases from 10ms to 500ms\n",
    "  - Users see timeouts\n",
    "  - Requests queue up\n",
    "\n",
    "Causes:\n",
    "  - Model not optimized (too complex)\n",
    "  - Features not cached\n",
    "  - CPU contention (other processes competing)\n",
    "  - GIL (Python Global Interpreter Lock)\n",
    "\n",
    "Solutions:\n",
    "  1. Profile inference (time each step)\n",
    "  2. Optimize model (quantization, pruning)\n",
    "  3. Add request queue (celery, message broker)\n",
    "  4. Use async inference (ProcessPoolExecutor)\n",
    "  5. Scale horizontally (more pods via HPA)\n",
    "```\n",
    "\n",
    "**3. ECR Credential Expiration**\n",
    "```\n",
    "Problem:\n",
    "  - IRSA token expires or becomes invalid\n",
    "  - New pods cannot pull image\n",
    "  - Deployments hang (Pending status)\n",
    "\n",
    "Prevention:\n",
    "  - Kubernetes/AWS handle token refresh automatically\n",
    "  - Just ensure IRSA is properly configured\n",
    "\n",
    "Debugging:\n",
    "  kubectl describe pod <pod-name>\n",
    "  # Look for: Failed to pull image \"...\": rpc error\n",
    "  \n",
    "  # Check IRSA role\n",
    "  aws iam get-role-policy \\\n",
    "    --role-name ml-inference-pod-role \\\n",
    "    --policy-name ml-inference-pod-policy\n",
    "```\n",
    "\n",
    "**4. Node Failure**\n",
    "```\n",
    "Problem:\n",
    "  - Hardware fails (disk, memory, network)\n",
    "  - All pods on node become unreachable\n",
    "  - ~5 minute detection + restart (pod-eviction-timeout)\n",
    "\n",
    "Prevention (HA Setup):\n",
    "  - Multi-AZ nodes (spread pods across nodes)\n",
    "  - Pod anti-affinity (pods don't share nodes)\n",
    "  - Pod Disruption Budget (min replicas)\n",
    "  - Our setup has all these! ✓\n",
    "\n",
    "What Happens:\n",
    "  1. Kubernetes detects node unhealthy\n",
    "  2. Marks pods as Terminating\n",
    "  3. Pods evicted to other nodes\n",
    "  4. HPA scales up if needed\n",
    "  5. Service continues (minimal impact)\n",
    "\n",
    "Recovery Time: ~60-120 seconds\n",
    "User Impact: <1% requests fail (if unlucky with timing)\n",
    "```\n",
    "\n",
    "**5. DDoS / Traffic Spike**\n",
    "```\n",
    "Problem:\n",
    "  - Sudden 10x traffic increase\n",
    "  - Pods max out (high CPU/latency)\n",
    "  - Users see 503 errors\n",
    "\n",
    "Automatic Mitigation (HPA):\n",
    "  - CPU jumps to 85%\n",
    "  - HPA detects in 30 seconds\n",
    "  - Scales to 10 pods immediately\n",
    "  - Cluster autoscaler adds nodes\n",
    "  - Load distributed\n",
    "  - Latency returns to normal\n",
    "\n",
    "Additional Mitigation:\n",
    "  - AWS WAF (block malicious IPs)\n",
    "  - Rate limiting (per IP limit)\n",
    "  - API Gateway throttling\n",
    "  - Circuit breaker pattern\n",
    "\n",
    "Time to Recover: 1-3 minutes\n",
    "```\n",
    "\n",
    "**6. Database Connection Exhaustion**\n",
    "```\n",
    "Problem:\n",
    "  - DB has 100 connections max\n",
    "  - 10 pods × 15 connections/pod = 150 (exceeds!)\n",
    "  - New requests fail with \"connection pool exhausted\"\n",
    "\n",
    "Prevention:\n",
    "  1. Connection pooling (HikariCP, SQLAlchemy pool)\n",
    "  2. Limit connections per pod (pool_size=5)\n",
    "  3. Scale down pods if DB overloaded\n",
    "  \n",
    "  # In app:\n",
    "  from sqlalchemy.pool import NullPool\n",
    "  engine = create_engine(db_url, poolclass=NullPool)\n",
    "  # Don't maintain persistent connections\n",
    "\n",
    "Detection:\n",
    "  - Monitor DB connection count\n",
    "  - Alert if > 80% of max\n",
    "```\n",
    "\n",
    "**7. Bad Model Update**\n",
    "```\n",
    "Problem:\n",
    "  - Deploy new model v2.0\n",
    "  - Accuracy is actually worse (overfitting)\n",
    "  - Business impact: wrong predictions\n",
    "\n",
    "Prevention:\n",
    "  1. Pre-deployment validation\n",
    "     - Test on hold-out test set\n",
    "     - Compare metrics vs old model\n",
    "     - Require explicit approval\n",
    "  \n",
    "  2. Canary deployment\n",
    "     - Deploy to 10% of traffic\n",
    "     - Monitor predictions (logging)\n",
    "     - Compare accuracy vs baseline\n",
    "     - If good: rollout to 100%\n",
    "  \n",
    "  3. Staged deployment\n",
    "     - Dev → Staging → Production\n",
    "     - Real validation at each stage\n",
    "\n",
    "Rollback (if needed):\n",
    "  kubectl rollout undo deployment/ml-inference-api -n ml-inference\n",
    "  # Instantly reverts to previous version\n",
    "  # Time to recover: 30-60 seconds\n",
    "```\n",
    "\n",
    "### Monitoring & Alerting Strategy:\n",
    "\n",
    "```yaml\n",
    "Key Metrics to Monitor:\n",
    "  1. Pod health\n",
    "     - Pods in CrashLoopBackOff\n",
    "     - Restart count > 10\n",
    "  \n",
    "  2. Resource usage\n",
    "     - CPU > 80%\n",
    "     - Memory > 85%\n",
    "     - Disk > 90%\n",
    "  \n",
    "  3. API performance\n",
    "     - Latency p95 > 100ms\n",
    "     - Error rate > 1%\n",
    "     - Requests/second trending\n",
    "  \n",
    "  4. Model quality\n",
    "     - Prediction confidence < 50% (drift)\n",
    "     - Distribution shift detected\n",
    "  \n",
    "  5. Cluster health\n",
    "     - Nodes NotReady\n",
    "     - Pod eviction rate high\n",
    "     - Pending pods (resource starved)\n",
    "\n",
    "Alerting Rules (examples):\n",
    "  - PodCrashLooping: Restart immediately\n",
    "  - HighMemoryUsage: Investigate leak\n",
    "  - HighAPILatency: Scale out or optimize\n",
    "  - HighErrorRate: Page on-call engineer\n",
    "```\n",
    "\n",
    "### Follow-up:\n",
    "- How would you implement automated rollback on error rate?\n",
    "- What's the RPO/RTO for your deployment?\n",
    "- How would you test disaster recovery scenarios?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a3d5ea",
   "metadata": {},
   "source": [
    "# 9. SUMMARY & KEY TAKEAWAYS\n",
    "\n",
    "## What We've Covered:\n",
    "\n",
    "### Kubernetes Fundamentals\n",
    "✓ Deployment vs StatefulSet vs DaemonSet  \n",
    "✓ Services (ClusterIP, NodePort, LoadBalancer)  \n",
    "✓ Ingress vs LoadBalancer  \n",
    "✓ Health checks (Startup, Liveness, Readiness)  \n",
    "\n",
    "### Advanced Kubernetes\n",
    "✓ HorizontalPodAutoscaler (pod scaling)  \n",
    "✓ PodDisruptionBudget (HA guarantees)  \n",
    "✓ Zero-downtime deployments  \n",
    "✓ Resource management (requests/limits)  \n",
    "\n",
    "### AWS & Infrastructure\n",
    "✓ IRSA (IAM Roles for Service Accounts)  \n",
    "✓ Terraform state management  \n",
    "✓ VPC design (public/private subnets)  \n",
    "✓ Security groups & networking  \n",
    "\n",
    "### Production Deployment\n",
    "✓ Deployment process (Terraform → Kubernetes)  \n",
    "✓ Debugging pod issues  \n",
    "✓ Monitoring & observability  \n",
    "✓ Handling production failures  \n",
    "\n",
    "### ML Specific\n",
    "✓ Model versioning strategies  \n",
    "✓ Inference request flow  \n",
    "✓ Latency optimization  \n",
    "✓ Model quality monitoring  \n",
    "\n",
    "## Key Architecture Principles:\n",
    "\n",
    "1. **High Availability**\n",
    "   - Multi-AZ deployment\n",
    "   - Multiple replicas (2-10)\n",
    "   - Pod anti-affinity\n",
    "   - Health checks\n",
    "\n",
    "2. **Auto-Scaling**\n",
    "   - Pod level (HPA: CPU/Memory)\n",
    "   - Node level (Cluster Autoscaler)\n",
    "   - Respects min/max limits\n",
    "\n",
    "3. **Security**\n",
    "   - IRSA (no hardcoded credentials)\n",
    "   - RBAC (least privilege)\n",
    "   - NetworkPolicy (traffic control)\n",
    "   - Pod security context\n",
    "\n",
    "4. **Observability**\n",
    "   - CloudWatch Container Insights\n",
    "   - Centralized logging\n",
    "   - Prometheus metrics\n",
    "   - Health checks\n",
    "\n",
    "5. **Reliability**\n",
    "   - Zero-downtime deployments\n",
    "   - Graceful shutdown (30s grace period)\n",
    "   - Pod Disruption Budgets\n",
    "   - Automated recovery\n",
    "\n",
    "## Interview Tips:\n",
    "\n",
    "1. **Show understanding of trade-offs**\n",
    "   - Why EKS vs Fargate?\n",
    "   - Why 2-10 replicas vs fixed?\n",
    "   - Why maxUnavailable: 0?\n",
    "\n",
    "2. **Connect to business value**\n",
    "   - \"This ensures 99.9% uptime\"\n",
    "   - \"Reduces MTTR from 30min to 1min\"\n",
    "   - \"Scales automatically with traffic\"\n",
    "\n",
    "3. **Demonstrate hands-on knowledge**\n",
    "   - Share debugging experience\n",
    "   - Explain error messages (not just copy-paste)\n",
    "   - Reference your actual setup\n",
    "\n",
    "4. **Ask clarifying questions**\n",
    "   - \"What's the expected traffic pattern?\"\n",
    "   - \"What's the acceptable downtime?\"\n",
    "   - \"What's the cost constraint?\"\n",
    "\n",
    "5. **Discuss improvements**\n",
    "   - \"We could add Prometheus for detailed metrics\"\n",
    "   - \"We should implement canary deployments\"\n",
    "   - \"We need automated rollback on error rate\"\n",
    "\n",
    "## Your Competitive Advantage:\n",
    "\n",
    "You have **complete, production-grade infrastructure code**:\n",
    "- 2,500+ lines of Terraform & Kubernetes\n",
    "- Real ML model deployment (not toy example)\n",
    "- Addresses actual production concerns:\n",
    "  - Security (IRSA, RBAC, NetworkPolicy)\n",
    "  - Reliability (health checks, PDB, multi-AZ)\n",
    "  - Scalability (HPA, cluster autoscaling)\n",
    "  - Observability (CloudWatch, logs)\n",
    "\n",
    "This is **far beyond typical interviews** which usually have minimal infrastructure.\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your interviews! This architecture demonstrates senior-level cloud engineering skills.** 🚀"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
